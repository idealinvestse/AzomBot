<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" class="logo" width="120"/>

# AZOM Pipeline Server - Komplett Implementation

H√§r √§r den kompletta implementationen av AZOM:s avancerade pipeline server, optimerad f√∂r OpenWebUI-integration med alla forskningsresultat implementerade.

## **Projektstruktur**

```
azom_pipeline_server/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_models.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ azom_models.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ orchestration_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ azom_knowledge_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ safety_service.py
‚îÇ   ‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_pipeline.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ azom_installation_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ swedish_nlp.py
‚îÇ       ‚îî‚îÄ‚îÄ cache_manager.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ knowledge_base/
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îî‚îÄ‚îÄ cache/
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```


## **app/config.py**

```python
import os
from pathlib import Path
from pydantic_settings import BaseSettings
from typing import List, Dict, Any

class Settings(BaseSettings):
    # Server Configuration
    APP_NAME: str = "AZOM Pipeline Server"
    APP_VERSION: str = "1.0.0"
    HOST: str = "0.0.0.0"
    PORT: int = 8001
    DEBUG: bool = False
    
    # OpenAI API Compatibility
    OPENAI_API_BASE: str = "http://titanic.urem.org:3000"
    OPENAI_API_KEY: str = "your_openwebui_api_key"
    TARGET_MODEL: str = "azom-se-general"
    
    # Pipeline Configuration
    MAX_CONCURRENT_REQUESTS: int = 10
    REQUEST_TIMEOUT: int = 300
    CACHE_TTL: int = 3600
    
    # AZOM Knowledge Base
    KNOWLEDGE_BASE_PATH: Path = Path("data/knowledge_base")
    EMBEDDINGS_PATH: Path = Path("data/embeddings")
    ENABLE_RAG: bool = True
    
    # Swedish NLP
    SWEDISH_MODEL_PATH: str = "KBLab/sentence-bert-swedish-cased"
    ENABLE_TRANSLATION: bool = True
    
    # Safety & Security
    MAX_TOKENS: int = 4000
    ENABLE_SAFETY_CHECKS: bool = True
    RATE_LIMIT_PER_IP: int = 100
    
    # Logging
    LOG_LEVEL: str = "INFO"
    LOG_FILE: str = "logs/azom_pipeline.log"
    
    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```


## **app/models/openai_models.py**

```python
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union
from enum import Enum

class Role(str, Enum):
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"

class Message(BaseModel):
    role: Role
    content: str
    name: Optional[str] = None
    tool_call_id: Optional[str] = None

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = Field(default=0.3, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(default=2000, ge=1, le=4000)
    top_p: Optional[float] = Field(default=1.0, ge=0.0, le=1.0)
    frequency_penalty: Optional[float] = Field(default=0.0, ge=-2.0, le=2.0)
    presence_penalty: Optional[float] = Field(default=0.0, ge=-2.0, le=2.0)
    stream: Optional[bool] = False
    user: Optional[str] = None

class Choice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Choice]
    usage: Usage
```


## **app/models/azom_models.py**

```python
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from enum import Enum

class UserExperience(str, Enum):
    BEGINNER = "Nyb√∂rjare"
    EXPERIENCED = "Erfaren" 
    PROFESSIONAL = "Professionell"

class InstallationComplexity(str, Enum):
    LOW = "L√•g"
    MEDIUM = "Medium"
    HIGH = "H√∂g"
    VERY_HIGH = "Mycket H√∂g"

class AZOMProduct(BaseModel):
    name: str
    price_sek: int
    compatibility: str
    complexity: InstallationComplexity
    success_rate: int
    installation_time: str
    canbus_required: bool
    canbus_code: Optional[str] = None
    features: List[str] = []

class CarAnalysis(BaseModel):
    brand: str
    model: str
    year: int
    group: str  # VW_Group, Premium, Asian, etc.
    canbus_support: bool
    complexity_base: InstallationComplexity
    azom_compatibility: str

class InstallationRecommendation(BaseModel):
    recommended_product: AZOMProduct
    alternative_products: List[AZOMProduct] = []
    installation_steps: List[str] = []
    safety_warnings: List[str] = []
    estimated_time: str
    success_probability: int
    professional_required: bool
    canbus_programming: Optional[Dict[str, str]] = None

class PipelineContext(BaseModel):
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    car_analysis: Optional[CarAnalysis] = None
    user_experience: Optional[UserExperience] = None
    previous_installations: List[Dict[str, Any]] = []
    safety_flags: List[str] = []
```


## **app/services/orchestration_service.py**

```python
import asyncio
from typing import Dict, Any, List, Optional
from langchain.schema.runnable import Runnable
from langchain.schema import BaseMessage, HumanMessage, SystemMessage
from app.models.azom_models import PipelineContext, UserExperience, CarAnalysis
from app.services.azom_knowledge_service import AZOMKnowledgeService
from app.services.rag_service import RAGService
from app.services.memory_service import MemoryService
from app.services.safety_service import SafetyService
from app.utils.swedish_nlp import SwedishNLP
from app.config import settings
import logging

logger = logging.getLogger(__name__)

class AZOMOrchestrationService:
    """Avancerad orchestration service f√∂r AZOM pipeline med LangChain integration."""
    
    def __init__(self):
        self.knowledge_service = AZOMKnowledgeService()
        self.rag_service = RAGService()
        self.memory_service = MemoryService()
        self.safety_service = SafetyService()
        self.swedish_nlp = SwedishNLP()
        
    async def process_azom_request(self, messages: List[BaseMessage], 
                                 context: PipelineContext) -> Dict[str, Any]:
        """Huvudorchestration f√∂r AZOM-f√∂rfr√•gningar."""
        
        try:
            # Steg 1: Analysera anv√§ndarinput
            user_intent = await self._analyze_user_intent(messages[-1].content)
            logger.info(f"User intent analyzed: {user_intent}")
            
            # Steg 2: Extrahera bilmodell och kontext
            car_analysis = await self._extract_car_information(messages[-1].content)
            if car_analysis:
                context.car_analysis = car_analysis
                logger.info(f"Car analysis: {car_analysis.brand} {car_analysis.model}")
            
            # Steg 3: H√§mta anv√§ndarminnen
            if context.user_id:
                context.previous_installations = await self.memory_service.get_user_history(context.user_id)
            
            # Steg 4: S√§kerhetsanalys
            safety_assessment = await self.safety_service.assess_safety_risk(
                user_input=messages[-1].content,
                car_analysis=car_analysis,
                user_experience=context.user_experience
            )
            context.safety_flags = safety_assessment.get("flags", [])
            
            # Steg 5: RAG-f√∂rst√§rkt kunskapss√∂kning
            relevant_knowledge = await self.rag_service.search_knowledge(
                query=messages[-1].content,
                car_model=car_analysis.model if car_analysis else None,
                user_experience=context.user_experience
            )
            
            # Steg 6: Generera AZOM-rekommendation
            azom_recommendation = await self.knowledge_service.generate_recommendation(
                car_analysis=car_analysis,
                user_experience=context.user_experience,
                user_input=messages[-1].content,
                relevant_knowledge=relevant_knowledge
            )
            
            # Steg 7: Bygg f√∂rst√§rkt systemprompt
            enhanced_system_prompt = await self._build_enhanced_system_prompt(
                context=context,
                recommendation=azom_recommendation,
                safety_flags=safety_assessment
            )
            
            # Steg 8: F√∂rst√§rk anv√§ndarmeddelande
            enhanced_user_message = await self._enhance_user_message(
                original_message=messages[-1].content,
                context=context,
                recommendation=azom_recommendation
            )
            
            return {
                "enhanced_system_prompt": enhanced_system_prompt,
                "enhanced_user_message": enhanced_user_message,
                "azom_recommendation": azom_recommendation,
                "safety_assessment": safety_assessment,
                "context": context,
                "relevant_knowledge": relevant_knowledge
            }
            
        except Exception as e:
            logger.error(f"Orchestration error: {e}")
            return await self._generate_fallback_response(e, context)
    
    async def _analyze_user_intent(self, user_input: str) -> Dict[str, Any]:
        """Analysera anv√§ndarens avsikt och behov."""
        
        intent_keywords = {
            "installation": ["installera", "montera", "s√§tta", "koppla"],
            "problem": ["fungerar inte", "problem", "fel", "trasig", "hj√§lp"],
            "product_inquiry": ["vilken", "rekommendera", "b√§st", "passa", "k√∂pa"],
            "compatibility": ["kompatibel", "passar", "fungerar med", "st√∂der"],
            "safety": ["s√§kert", "risk", "skada", "farligt", "varning"]
        }
        
        detected_intents = []
        confidence_scores = {}
        
        user_input_lower = user_input.lower()
        
        for intent, keywords in intent_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in user_input_lower)
            if matches > 0:
                detected_intents.append(intent)
                confidence_scores[intent] = matches / len(keywords)
        
        return {
            "primary_intent": detected_intents[0] if detected_intents else "general",
            "all_intents": detected_intents,
            "confidence_scores": confidence_scores,
            "requires_car_analysis": any(intent in detected_intents for intent in ["installation", "compatibility", "product_inquiry"])
        }
    
    async def _extract_car_information(self, user_input: str) -> Optional[CarAnalysis]:
        """Extrahera bilmodell och √•rsmodell fr√•n anv√§ndarinput."""
        
        # Anv√§nd svensk NLP f√∂r att identifiera bilmodeller
        car_entities = await self.swedish_nlp.extract_car_entities(user_input)
        
        if not car_entities:
            return None
        
        # Grundl√§ggande mappning av bilm√§rken till grupper
        brand_mapping = {
            "volkswagen": {"group": "VW_Group", "canbus": True, "complexity": "Medium"},
            "golf": {"group": "VW_Group", "canbus": True, "complexity": "Medium", "brand": "Volkswagen"},
            "passat": {"group": "VW_Group", "canbus": True, "complexity": "High", "brand": "Volkswagen"},
            "skoda": {"group": "VW_Group", "canbus": True, "complexity": "Medium"},
            "seat": {"group": "VW_Group", "canbus": True, "complexity": "Medium"},
            "bmw": {"group": "Premium", "canbus": True, "complexity": "Very_High"},
            "mercedes": {"group": "Premium", "canbus": True, "complexity": "Very_High"},
            "audi": {"group": "Premium", "canbus": True, "complexity": "High"},
            "toyota": {"group": "Asian", "canbus": False, "complexity": "Low"},
            "honda": {"group": "Asian", "canbus": False, "complexity": "Low"}
        }
        
        # Ta f√∂rsta detekterade bil
        car_entity = car_entities[0]
        brand_info = brand_mapping.get(car_entity["brand"].lower(), {
            "group": "Other", "canbus": False, "complexity": "Low"
        })
        
        return CarAnalysis(
            brand=brand_info.get("brand", car_entity["brand"]),
            model=car_entity["model"],
            year=car_entity.get("year", 2015),  # Default √•r
            group=brand_info["group"],
            canbus_support=brand_info["canbus"],
            complexity_base=brand_info["complexity"],
            azom_compatibility="Excellent" if brand_info["group"] == "VW_Group" else "Limited"
        )
    
    async def _build_enhanced_system_prompt(self, context: PipelineContext, 
                                          recommendation: Dict[str, Any],
                                          safety_flags: Dict[str, Any]) -> str:
        """Bygg en f√∂rst√§rkt systemprompt med AZOM-kontext."""
        
        base_prompt = """Du √§r AZOM Installations-Expert Pro, Skandinaviens mest avancerade AI-specialist f√∂r installation av AZOM multimediaspelare.

AKTUELL SITUATION OCH KONTEXT:"""
        
        # L√§gg till bilanalys om tillg√§nglig
        if context.car_analysis:
            base_prompt += f"""
üöó BILANALYS:
- M√§rke/Modell: {context.car_analysis.brand} {context.car_analysis.model} ({context.car_analysis.year})
- Kompatibilitetsgrupp: {context.car_analysis.group}
- CANBUS-st√∂d: {'Ja' if context.car_analysis.canbus_support else 'Nej'}
- Grundkomplexitet: {context.car_analysis.complexity_base}
- AZOM-kompatibilitet: {context.car_analysis.azom_compatibility}"""
        
        # L√§gg till produktrekommendation
        if recommendation.get("recommended_product"):
            product = recommendation["recommended_product"]
            base_prompt += f"""

üí° REKOMMENDERAD AZOM-PRODUKT:
- Produkt: {product.get('name', 'N/A')} ({product.get('price_sek', 'N/A')} SEK)
- Framg√•ngsfrekvens: {product.get('success_rate', 'N/A')}%
- Installationstid: {product.get('installation_time', 'N/A')}
- CANBUS kr√§vs: {'Ja' if product.get('canbus_required') else 'Nej'}"""
        
        # L√§gg till s√§kerhetsvarningar
        if safety_flags.get("flags"):
            base_prompt += f"""

‚ö†Ô∏è S√ÑKERHETSVARNINGAR:
{chr(10).join(['- ' + flag for flag in safety_flags["flags"]])}"""
        
        # L√§gg till anv√§ndarhistorik
        if context.previous_installations:
            successful = len([i for i in context.previous_installations if i.get("success")])
            base_prompt += f"""

üìä ANV√ÑNDARHISTORIK:
- Tidigare installationer: {len(context.previous_installations)}
- Framg√•ngsrika: {successful}
- Erfarenhetsniv√•: {context.user_experience or 'Ok√§nd'}"""
        
        base_prompt += """

INSTRUKTIONER:
1. Ge konkreta, s√§kra installationsinstruktioner p√• svenska
2. Anpassa komplexitetsniv√•n till anv√§ndarens erfarenhet
3. Prioritera s√§kerhet √∂ver snabbhet
4. Inkludera tydliga n√§sta steg och kontaktinformation
5. Anv√§nd emojis f√∂r visuell tydlighet (üîã‚ö†Ô∏èüì∏üîß)

Svara alltid med trygg expertis som bygger f√∂rtroende och s√§kerst√§ller framg√•ngsrik installation."""
        
        return base_prompt
    
    async def _enhance_user_message(self, original_message: str, 
                                  context: PipelineContext,
                                  recommendation: Dict[str, Any]) -> str:
        """F√∂rst√§rk anv√§ndarmeddelande med kontext och rekommendationer."""
        
        enhanced = f"""ANV√ÑNDARENS URSPRUNGLIGA F√ñRFR√ÖGAN:
{original_message}

F√ñRST√ÑRKT KONTEXT FR√ÖN AZOM PIPELINE:"""
        
        if context.car_analysis:
            enhanced += f"""
- Detekterad bil: {context.car_analysis.brand} {context.car_analysis.model}
- Kompatibilitetsanalys: {context.car_analysis.azom_compatibility}"""
        
        if recommendation.get("recommended_product"):
            enhanced += f"""
- Rekommenderad produkt: {recommendation['recommended_product'].get('name')}
- Framg√•ngsprognos: {recommendation['recommended_product'].get('success_rate')}%"""
        
        if context.user_experience:
            enhanced += f"""
- Anv√§ndarens erfarenhetsniv√•: {context.user_experience}"""
        
        enhanced += """

Basera ditt svar p√• denna f√∂rst√§rkta kontext och ge en detaljerad, trygg v√§gledning."""
        
        return enhanced
    
    async def _generate_fallback_response(self, error: Exception, 
                                        context: PipelineContext) -> Dict[str, Any]:
        """Generera s√§ker fallback-respons vid fel."""
        
        fallback_prompt = """Du √§r AZOM Installations-Expert. Systemet har tempor√§ra problem men du kan √§nd√• hj√§lpa kunden s√§kert.

FALLBACK-PROTOKOLL:
1. Erk√§nn att systemet har tempor√§ra problem
2. Ge grundl√§ggande s√§kerhetsr√•d
3. Rekommendera DNA Universal f√∂r ok√§nda situationer
4. H√§nvisa till AZOM support: support@azom.se
5. Betona s√§kerhet framf√∂r allt annat"""
        
        fallback_message = """Jag har tillf√§lliga systemproblem men kan √§nd√• hj√§lpa dig s√§kert.

F√∂r ok√§nda bilmodeller rekommenderar jag alltid:
- AZOM DNA Universal (2199 SEK) - fungerar i alla bilar
- 95% framg√•ngsfrekvens f√∂r nyb√∂rjare
- Universal kompatibilitet utan CANBUS-risk

Kontakta AZOM support f√∂r detaljerad hj√§lp: support@azom.se"""
        
        return {
            "enhanced_system_prompt": fallback_prompt,
            "enhanced_user_message": fallback_message,
            "azom_recommendation": {"fallback": True},
            "safety_assessment": {"flags": ["System temporarily degraded"]},
            "context": context,
            "error": str(error)
        }
```


## **app/services/rag_service.py**

```python
import asyncio
from typing import List, Dict, Any, Optional
from pathlib import Path
import pandas as pd
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import json
from app.config import settings
import logging

logger = logging.getLogger(__name__)

class RAGService:
    """Retrieval-Augmented Generation service f√∂r AZOM kunskapsbas."""
    
    def __init__(self):
        self.model = None
        self.embeddings_cache = {}
        self.knowledge_documents = []
        self.embeddings_matrix = None
        self._load_model()
        
    def _load_model(self):
        """Ladda svensk sentence transformer model."""
        try:
            self.model = SentenceTransformer(settings.SWEDISH_MODEL_PATH)
            logger.info(f"Loaded Swedish embedding model: {settings.SWEDISH_MODEL_PATH}")
        except Exception as e:
            logger.warning(f"Failed to load Swedish model, falling back to multilingual: {e}")
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    async def initialize_knowledge_base(self):
        """Initialisera och bygg embeddings f√∂r kunskapsdatabasen."""
        
        try:
            # Ladda alla kunskapsdokument
            await self._load_knowledge_documents()
            
            # Kontrollera om cached embeddings finns
            embeddings_file = settings.EMBEDDINGS_PATH / "azom_embeddings.pkl"
            
            if embeddings_file.exists():
                await self._load_cached_embeddings(embeddings_file)
            else:
                await self._generate_embeddings()
                await self._save_embeddings(embeddings_file)
            
            logger.info(f"RAG service initialized with {len(self.knowledge_documents)} documents")
            
        except Exception as e:
            logger.error(f"Failed to initialize RAG service: {e}")
    
    async def _load_knowledge_documents(self):
        """Ladda alla kunskapsdokument fr√•n AZOM kunskapsbas."""
        
        self.knowledge_documents = []
        knowledge_path = settings.KNOWLEDGE_BASE_PATH
        
        # Ladda CSV-filer
        for csv_file in knowledge_path.glob("*.csv"):
            try:
                df = pd.read_csv(csv_file)
                for _, row in df.iterrows():
                    doc = {
                        "id": f"{csv_file.stem}_{len(self.knowledge_documents)}",
                        "source": csv_file.name,
                        "type": "structured_data",
                        "content": self._csv_row_to_text(row),
                        "metadata": row.to_dict()
                    }
                    self.knowledge_documents.append(doc)
            except Exception as e:
                logger.error(f"Failed to load CSV {csv_file}: {e}")
        
        # Ladda Markdown-filer
        for md_file in knowledge_path.glob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Dela upp i chunks om dokumentet √§r stort
                chunks = self._split_markdown_content(content)
                
                for i, chunk in enumerate(chunks):
                    doc = {
                        "id": f"{md_file.stem}_chunk_{i}",
                        "source": md_file.name,
                        "type": "markdown",
                        "content": chunk,
                        "metadata": {"chunk_index": i, "total_chunks": len(chunks)}
                    }
                    self.knowledge_documents.append(doc)
                    
            except Exception as e:
                logger.error(f"Failed to load Markdown {md_file}: {e}")
        
        # Ladda JSON-konfigurationsfiler
        for json_file in knowledge_path.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                doc = {
                    "id": f"{json_file.stem}_config",
                    "source": json_file.name,
                    "type": "configuration",
                    "content": self._json_to_text(data),
                    "metadata": data
                }
                self.knowledge_documents.append(doc)
                
            except Exception as e:
                logger.error(f"Failed to load JSON {json_file}: {e}")
    
    def _csv_row_to_text(self, row: pd.Series) -> str:
        """Konvertera CSV-rad till s√∂kbar text."""
        text_parts = []
        
        for column, value in row.items():
            if pd.notna(value):
                text_parts.append(f"{column}: {value}")
        
        return " | ".join(text_parts)
    
    def _split_markdown_content(self, content: str, max_chunk_size: int = 1000) -> List[str]:
        """Dela upp Markdown-inneh√•ll i s√∂kbara chunks."""
        
        # Dela p√• headers f√∂rst
        sections = content.split('\n#')
        chunks = []
        
        for section in sections:
            if len(section) <= max_chunk_size:
                chunks.append(section.strip())
            else:
                # Dela l√§ngre sektioner p√• stycken
                paragraphs = section.split('\n\n')
                current_chunk = ""
                
                for paragraph in paragraphs:
                    if len(current_chunk + paragraph) <= max_chunk_size:
                        current_chunk += paragraph + "\n\n"
                    else:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = paragraph + "\n\n"
                
                if current_chunk:
                    chunks.append(current_chunk.strip())
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 50]  # Filtrera f√∂r korta chunks
    
    def _json_to_text(self, data: Dict[str, Any]) -> str:
        """Konvertera JSON-data till s√∂kbar text."""
        def extract_text(obj, prefix=""):
            texts = []
            
            if isinstance(obj, dict):
                for key, value in obj.items():
                    new_prefix = f"{prefix}.{key}" if prefix else key
                    texts.extend(extract_text(value, new_prefix))
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    texts.extend(extract_text(item, f"{prefix}[{i}]"))
            else:
                texts.append(f"{prefix}: {obj}")
            
            return texts
        
        return " | ".join(extract_text(data))
    
    async def _generate_embeddings(self):
        """Generera embeddings f√∂r alla kunskapsdokument."""
        
        logger.info("Generating embeddings for knowledge base...")
        
        texts = [doc["content"] for doc in self.knowledge_documents]
        
        # Generera embeddings i batches f√∂r att hantera stora dataset
        batch_size = 32
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)
            all_embeddings.extend(batch_embeddings)
        
        self.embeddings_matrix = np.array(all_embeddings)
        logger.info(f"Generated embeddings for {len(texts)} documents")
    
    async def _load_cached_embeddings(self, embeddings_file: Path):
        """Ladda cachade embeddings."""
        
        with open(embeddings_file, 'rb') as f:
            cache_data = pickle.load(f)
        
        self.embeddings_matrix = cache_data["embeddings"]
        cached_docs = cache_data["documents"]
        
        # Verifiera att cachade dokument matchar nuvarande
        if len(cached_docs) == len(self.knowledge_documents):
            logger.info("Loaded cached embeddings successfully")
        else:
            logger.warning("Cached embeddings don't match current documents, regenerating...")
            await self._generate_embeddings()
    
    async def _save_embeddings(self, embeddings_file: Path):
        """Spara embeddings till cache."""
        
        embeddings_file.parent.mkdir(parents=True, exist_ok=True)
        
        cache_data = {
            "embeddings": self.embeddings_matrix,
            "documents": [doc["id"] for doc in self.knowledge_documents]
        }
        
        with open(embeddings_file, 'wb') as f:
            pickle.dump(cache_data, f)
        
        logger.info(f"Saved embeddings cache to {embeddings_file}")
    
    async def search_knowledge(self, query: str, car_model: Optional[str] = None,
                             user_experience: Optional[str] = None,
                             top_k: int = 5) -> List[Dict[str, Any]]:
        """S√∂k relevant kunskap baserat p√• anv√§ndarfr√•ga."""
        
        if not self.model or self.embeddings_matrix is None:
            logger.warning("RAG service not initialized, returning empty results")
            return []
        
        try:
            # F√∂rst√§rk fr√•gan med kontext
            enhanced_query = self._enhance_query(query, car_model, user_experience)
            
            # Generera embedding f√∂r fr√•gan
            query_embedding = self.model.encode([enhanced_query])
            
            # Ber√§kna similarity med alla dokument
            similarities = cosine_similarity(query_embedding, self.embeddings_matrix)[0]
            
            # H√§mta top-k mest relevanta dokument
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.3:  # Threshold f√∂r relevans
                    doc = self.knowledge_documents[idx].copy()
                    doc["similarity_score"] = float(similarities[idx])
                    results.append(doc)
            
            # Filtrera och rankning baserat p√• bilmodell om specificerat
            if car_model:
                results = self._filter_by_car_model(results, car_model)
            
            logger.info(f"RAG search returned {len(results)} relevant documents for query: {query[:50]}...")
            return results
            
        except Exception as e:
            logger.error(f"RAG search failed: {e}")
            return []
    
    def _enhance_query(self, query: str, car_model: Optional[str] = None,
                      user_experience: Optional[str] = None) -> str:
        """F√∂rst√§rk s√∂kfr√•gan med kontextuell information."""
        
        enhanced_parts = [query]
        
        if car_model:
            enhanced_parts.append(f"bilmodell {car_model}")
        
        if user_experience:
            enhanced_parts.append(f"erfarenhetsniv√• {user_experience}")
        
        # L√§gg till AZOM-specifika termer f√∂r b√§ttre matching
        azom_terms = ["AZOM", "installation", "multimediaspelare", "CANBUS"]
        enhanced_parts.extend(azom_terms)
        
        return " ".join(enhanced_parts)
    
    def _filter_by_car_model(self, results: List[Dict[str, Any]], 
                           car_model: str) -> List[Dict[str, Any]]:
        """Filtrera och prioritera resultat baserat p√• bilmodell."""
        
        car_model_lower = car_model.lower()
        
        # Dela upp i modell-specifika och generella resultat
        specific_results = []
        general_results = []
        
        for result in results:
            content_lower = result["content"].lower()
            metadata = result.get("metadata", {})
            
            # Kontrollera om dokumentet √§r specifikt f√∂r bilmodellen
            is_specific = (
                car_model_lower in content_lower or
                any(car_model_lower in str(value).lower() for value in metadata.values())
            )
            
            if is_specific:
                specific_results.append(result)
            else:
                general_results.append(result)
        
        # Returnera specifika resultat f√∂rst, sedan generella
        return specific_results + general_results[:max(0, 5 - len(specific_results))]
    
    def get_knowledge_statistics(self) -> Dict[str, Any]:
        """H√§mta statistik √∂ver kunskapsdatabasen."""
        
        stats = {
            "total_documents": len(self.knowledge_documents),
            "document_types": {},
            "sources": set(),
            "embeddings_ready": self.embeddings_matrix is not None
        }
        
        for doc in self.knowledge_documents:
            doc_type = doc["type"]
            stats["document_types"][doc_type] = stats["document_types"].get(doc_type, 0) + 1
            stats["sources"].add(doc["source"])
        
        stats["sources"] = list(stats["sources"])
        
        return stats
```


## **app/pipelines/azom_installation_pipeline.py**

```python
import asyncio
import json
import time
import uuid
from typing import Dict, Any, List, Optional
from fastapi import HTTPException
from app.models.openai_models import ChatCompletionRequest, ChatCompletionResponse, Choice, Message, Usage
from app.models.azom_models import PipelineContext, UserExperience
from app.services.orchestration_service import AZOMOrchestrationService
from app.services.memory_service import MemoryService
from app.config import settings
import httpx
import logging

logger = logging.getLogger(__name__)

class AZOMInstallationPipeline:
    """Huvudpipeline f√∂r AZOM installation guidance med OpenAI API-kompatibilitet."""
    
    def __init__(self):
        self.orchestration_service = AZOMOrchestrationService()
        self.memory_service = MemoryService()
        self.client = httpx.AsyncClient(timeout=settings.REQUEST_TIMEOUT)
        
    async def process_chat_completion(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """Huvudmetod f√∂r att bearbeta chat completion requests."""
        
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        logger.info(f"Processing AZOM chat completion request {request_id}")
        
        try:
            # Extrahera anv√§ndarkontext
            context = await self._extract_pipeline_context(request)
            
            # K√∂r AZOM orchestration
            orchestration_result = await self.orchestration_service.process_azom_request(
                messages=self._convert_to_langchain_messages(request.messages),
                context=context
            )
            
            # Bygg f√∂rst√§rkt request f√∂r underliggande LLM
            enhanced_request = await self._build_enhanced_request(
                original_request=request,
                orchestration_result=orchestration_result
            )
            
            # Anropa underliggande LLM (azom-se-general)
            llm_response = await self._call_underlying_llm(enhanced_request)
            
            # Post-processing av LLM-svar
            final_response = await self._post_process_response(
                llm_response=llm_response,
                orchestration_result=orchestration_result,
                context=context
            )
            
            # Spara anv√§ndarinteraktion i minnet
            if context.user_id:
                await self._save_interaction_memory(
                    context=context,
                    request=request,
                    response=final_response,
                    orchestration_result=orchestration_result
                )
            
            processing_time = time.time() - start_time
            logger.info(f"AZOM pipeline completed in {processing_time:.2f}s for request {request_id}")
            
            return final_response
            
        except Exception as e:
            logger.error(f"AZOM pipeline error for request {request_id}: {e}")
            return await self._generate_error_response(request, str(e))
    
    async def _extract_pipeline_context(self, request: ChatCompletionRequest) -> PipelineContext:
        """Extrahera AZOM pipeline-kontext fr√•n request."""
        
        context = PipelineContext()
        
        # Extrahera user_id fr√•n request om tillg√§ngligt
        context.user_id = request.user
        context.session_id = str(uuid.uuid4())  # Generera session ID
        
        # Analysera meddelanden f√∂r anv√§ndarerfarenhet
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        if user_messages:
            latest_message = user_messages[-1].content.lower()
            
            # Enkel heuristik f√∂r att detektera erfarenhetsniv√•
            if any(word in latest_message for word in ["nyb√∂rjare", "f√∂rsta g√•ngen", "aldrig", "os√§ker"]):
                context.user_experience = UserExperience.BEGINNER
            elif any(word in latest_message for word in ["erfaren", "gjort f√∂rut", "vet hur"]):
                context.user_experience = UserExperience.EXPERIENCED
            elif any(word in latest_message for word in ["professionell", "tekniker", "expert"]):
                context.user_experience = UserExperience.PROFESSIONAL
        
        return context
    
    def _convert_to_langchain_messages(self, messages: List[Message]) -> List[Any]:
        """Konvertera OpenAI-meddelanden till LangChain-format."""
        
        from langchain.schema import HumanMessage, SystemMessage, AIMessage
        
        langchain_messages = []
        
        for msg in messages:
            if msg.role == "user":
                langchain_messages.append(HumanMessage(content=msg.content))
            elif msg.role == "system":
                langchain_messages.append(SystemMessage(content=msg.content))
            elif msg.role == "assistant":
                langchain_messages.append(AIMessage(content=msg.content))
        
        return langchain_messages
    
    async def _build_enhanced_request(self, original_request: ChatCompletionRequest,
                                    orchestration_result: Dict[str, Any]) -> ChatCompletionRequest:
        """Bygg f√∂rst√§rkt request med AZOM-kontext."""
        
        enhanced_messages = []
        
        # L√§gg till f√∂rst√§rkt systemprompt
        enhanced_system_message = Message(
            role="system",
            content=orchestration_result["enhanced_system_prompt"]
        )
        enhanced_messages.append(enhanced_system_message)
        
        # Kopiera befintliga meddelanden (exklusive system-meddelanden)
        for msg in original_request.messages:
            if msg.role != "system":
                enhanced_messages.append(msg)
        
        # Ers√§tt det senaste anv√§ndarmeddelandet med f√∂rst√§rkt version
        if enhanced_messages and enhanced_messages[-1].role == "user":
            enhanced_messages[-1] = Message(
                role="user",
                content=orchestration_result["enhanced_user_message"]
            )
        
        # Skapa ny request med f√∂rst√§rkta meddelanden
        enhanced_request = ChatCompletionRequest(
            model=original_request.model,
            messages=enhanced_messages,
            temperature=original_request.temperature,
            max_tokens=min(original_request.max_tokens or 2000, settings.MAX_TOKENS),
            top_p=original_request.top_p,
            frequency_penalty=original_request.frequency_penalty,
            presence_penalty=original_request.presence_penalty,
            stream=False,  # S√§kerst√§ll att streaming √§r av f√∂r pipeline
            user=original_request.user
        )
        
        return enhanced_request
    
    async def _call_underlying_llm(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """Anropa underliggande LLM (azom-se-general) via OpenWebUI."""
        
        try:
            headers = {
                "Authorization": f"Bearer {settings.OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }
            
            payload = request.dict()
            
            response = await self.client.post(
                f"{settings.OPENAI_API_BASE}/api/chat/completions",
                headers=headers,
                json=payload
            )
            
            response.raise_for_status()
            response_data = response.json()
            
            return ChatCompletionResponse(**response_data)
            
        except httpx.HTTPError as e:
            logger.error(f"LLM API call failed: {e}")
            raise HTTPException(status_code=503, detail="Underliggande AI-modell inte tillg√§nglig")
        except Exception as e:
            logger.error(f"Unexpected error calling LLM: {e}")
            raise HTTPException(status_code=500, detail="Internt fel vid AI-anrop")
    
    async def _post_process_response(self, llm_response: ChatCompletionResponse,
                                   orchestration_result: Dict[str, Any],
                                   context: PipelineContext) -> ChatCompletionResponse:
        """Post-processing av LLM-svar med AZOM-specifika f√∂rb√§ttringar."""
        
        if not llm_response.choices:
            return llm_response
        
        # H√§mta det genererade svaret
        original_content = llm_response.choices[0].message.content
        
        # L√§gg till AZOM-specifika f√∂rb√§ttringar
        enhanced_content = await self._enhance_response_content(
            original_content=original_content,
            orchestration_result=orchestration_result,
            context=context
        )
        
        # Skapa f√∂rb√§ttrat svar
        enhanced_message = Message(
            role="assistant",
            content=enhanced_content
        )
        
        enhanced_choice = Choice(
            index=0,
            message=enhanced_message,
            finish_reason=llm_response.choices[0].finish_reason
        )
        
        # Ber√§kna token-anv√§ndning (approximation)
        enhanced_usage = Usage(
            prompt_tokens=llm_response.usage.prompt_tokens,
            completion_tokens=llm_response.usage.completion_tokens + 50,  # L√§gg till f√∂r enhancements
            total_tokens=llm_response.usage.total_tokens + 50
        )
        
        return ChatCompletionResponse(
            id=llm_response.id,
            object=llm_response.object,
            created=llm_response.created,
            model=f"azom-pipeline-{llm_response.model}",
            choices=[enhanced_choice],
            usage=enhanced_usage
        )
    
    async def _enhance_response_content(self, original_content: str,
                                      orchestration_result: Dict[str, Any],
                                      context: PipelineContext) -> str:
        """F√∂rb√§ttra svarsinneh√•ll med AZOM-specifika till√§gg."""
        
        enhanced_content = original_content
        
        # L√§gg till produktrekommendation om den saknas
        recommendation = orchestration_result.get("azom_recommendation", {})
        if recommendation and "rekommenderad produkt" not in original_content.lower():
            product_info = recommendation.get("recommended_product", {})
            if product_info:
                product_section = f"""

üí° **AZOM PRODUKTREKOMMENDATION:**
- **{product_info.get('name')}** ({product_info.get('price_sek')} SEK)
- Framg√•ngsfrekvens: {product_info.get('success_rate')}%
- Installationstid: {product_info.get('installation_time')}
- CANBUS-programmering: {'Kr√§vs' if product_info.get('canbus_required') else 'Ej n√∂dv√§ndig'}"""
                
                enhanced_content += product_section
        
        # L√§gg till s√§kerhetsvarningar om de saknas
        safety_flags = orchestration_result.get("safety_assessment", {}).get("flags", [])
        if safety_flags and "s√§kerhet" not in original_content.lower():
            safety_section = f"""

‚ö†Ô∏è **S√ÑKERHETSVARNINGAR:**
{chr(10).join(['- ' + flag for flag in safety_flags])}"""
            
            enhanced_content += safety_section
        
        # L√§gg till n√§sta steg baserat p√• erfarenhetsniv√•
        if context.user_experience == UserExperience.BEGINNER:
            beginner_footer = """

üî∞ **N√ÑSTA STEG F√ñR NYB√ñRJARE:**
1. üìû Ring AZOM support om du √§r os√§ker: support@azom.se
2. üì∏ Ta bilder p√• befintlig installation INNAN du b√∂rjar
3. üîã Koppla ALLTID bort bilbatteriet f√∂rst
4. üõ†Ô∏è F√∂rbered alla verktyg innan du b√∂rjar
5. ‚è∞ Avs√§tt gott om tid - ingen br√•dska!"""
            
            enhanced_content += beginner_footer
        
        # L√§gg till kontaktinformation om den saknas
        if "support@azom.se" not in enhanced_content:
            contact_footer = """

üìû **AZOM SUPPORT:**
- E-post: support@azom.se
- Webbplats: azom.se
- Ring vid akut problem under installation"""
            
            enhanced_content += contact_footer
        
        return enhanced_content
    
    async def _save_interaction_memory(self, context: PipelineContext,
                                     request: ChatCompletionRequest,
                                     response: ChatCompletionResponse,
                                     orchestration_result: Dict[str, Any]):
        """Spara anv√§ndarinteraktion i minnet f√∂r framtida referens."""
        
        try:
            memory_data = {
                "timestamp": time.time(),
                "session_id": context.session_id,
                "user_input": request.messages[-1].content if request.messages else "",
                "car_analysis": orchestration_result.get("context", {}).get("car_analysis"),
                "recommended_product": orchestration_result.get("azom_recommendation", {}).get("recommended_product"),
                "user_experience": context.user_experience,
                "safety_flags": orchestration_result.get("safety_assessment", {}).get("flags", []),
                "response_length": len(response.choices[0].message.content) if response.choices else 0,
                "tokens_used": response.usage.total_tokens if response.usage else 0
            }
            
            await self.memory_service.store_interaction(context.user_id, memory_data)
            
        except Exception as e:
            logger.error(f"Failed to save interaction memory: {e}")
            # Forts√§tt √§ven om memory-lagring misslyckas
    
    async def _generate_error_response(self, request: ChatCompletionRequest, 
                                     error_message: str) -> ChatCompletionResponse:
        """Generera s√§kert fel-svar f√∂r anv√§ndaren."""
        
        safe_error_message = """üö® Jag har tempor√§ra tekniska problem, men din s√§kerhet √§r viktigast.

**S√ÑKER FALLBACK-REKOMMENDATION:**
- F√∂r ok√§nda bilar: AZOM DNA Universal (2199 SEK)
- 95% framg√•ngsfrekvens utan CANBUS-risk
- Universal kompatibilitet

**S√ÑKERHETSPROTOKOLL:**
üîã Koppla ALLTID bort bilbatteriet f√∂rst
üì∏ Ta bilder f√∂re nedmontering
üìû Ring AZOM support vid os√§kerhet: support@azom.se

Jag ber om urs√§kt f√∂r tekniska problem. AZOM support hj√§lper dig s√§kert!"""
        
        error_message_obj = Message(
            role="assistant",
            content=safe_error_message
        )
        
        error_choice = Choice(
            index=0,
            message=error_message_obj,
            finish_reason="error_fallback"
        )
        
        error_usage = Usage(
            prompt_tokens=0,
            completion_tokens=len(safe_error_message.split()),
            total_tokens=len(safe_error_message.split())
        )
        
        return ChatCompletionResponse(
            id=str(uuid.uuid4()),
            object="chat.completion",
            created=int(time.time()),
            model="azom-pipeline-error",
            choices=[error_choice],
            usage=error_usage
        )
    
    async def cleanup(self):
        """Rensa resurser vid shutdown."""
        await self.client.aclose()
```


## **app/main.py**

```python
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import asyncio
import logging
from contextlib import asynccontextmanager
from datetime import datetime

from app.config import settings
from app.models.openai_models import ChatCompletionRequest, ChatCompletionResponse
from app.pipelines.azom_installation_pipeline import AZOMInstallationPipeline
from app.services.rag_service import RAGService

# Setup logging
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL.upper()),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(settings.LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Global services
azom_pipeline = None
rag_service = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan."""
    # Startup
    global azom_pipeline, rag_service
    
    logger.info(f"Starting {settings.APP_NAME} v{settings.APP_VERSION}")
    
    try:
        # Initialize services
        azom_pipeline = AZOMInstallationPipeline()
        rag_service = RAGService()
        
        # Initialize RAG knowledge base
        logger.info("Initializing AZOM knowledge base...")
        await rag_service.initialize_knowledge_base()
        
        # Initialize orchestration service
        await azom_pipeline.orchestration_service.rag_service.initialize_knowledge_base()
        
        logger.info("AZOM Pipeline Server started successfully")
        
    except Exception as e:
        logger.error(f"Startup failed: {e}")
        raise
    
    yield
    
    # Shutdown
    logger.info("Shutting down AZOM Pipeline Server")
    
    try:
        if azom_pipeline:
            await azom_pipeline.cleanup()
        
        logger.info("Shutdown completed")
        
    except Exception as e:
        logger.error(f"Shutdown error: {e}")

# Initialize FastAPI app
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description="Avancerad Pipeline Server f√∂r AZOM AI-agent med svensk multimediaspelare-expertis",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request logging middleware
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = datetime.now()
    
    response = await call_next(request)
    
    process_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(
        f"{request.method} {request.url.path} - "
        f"{response.status_code} - {process_time:.3f}s"
    )
    
    return response

# OpenAI API Compatible Endpoints
@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def chat_completions(request: ChatCompletionRequest):
    """
    OpenAI-kompatibel chat completions endpoint f√∂r AZOM AI-agent.
    
    Denna endpoint processerar chat-f√∂rfr√•gningar genom AZOM:s avancerade
    pipeline som inkluderar bilanalys, produktrekommendationer och s√§kerhetsvalidering.
    """
    
    if not azom_pipeline:
        raise HTTPException(status_code=503, detail="Pipeline not initialized")
    
    try:
        logger.info(f"Processing chat completion for model: {request.model}")
        
        # Validera modell (acceptera alla f√∂r kompatibilitet)
        if not request.messages:
            raise HTTPException(status_code=400, detail="Messages cannot be empty")
        
        # Process through AZOM pipeline
        response = await azom_pipeline.process_chat_completion(request)
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat completion error: {e}")
        raise HTTPException(status_code=500, detail=f"Internal pipeline error: {str(e)}")

# Standard endpoints
@app.get("/")
async def root():
    """Root endpoint providing server information."""
    return {
        "service": settings.APP_NAME,
        "version": settings.APP_VERSION,
        "status": "operational",
        "description": "Avancerad Pipeline Server f√∂r AZOM multimediaspelare-installation",
        "openai_compatible": True,
        "endpoints": {
            "chat_completions": "/v1/chat/completions",
            "health": "/health",
            "stats": "/stats"
        },
        "azom": {
            "specialization": "VW/Skoda/Seat multimediaspelare",
            "support_email": "support@azom.se",
            "languages": ["svenska"]
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": settings.APP_VERSION,
        "services": {}
    }
    
    try:
        # Check pipeline status
        if azom_pipeline:
            health_status["services"]["azom_pipeline"] = "healthy"
        else:
            health_status["services"]["azom_pipeline"] = "unhealthy"
            health_status["status"] = "degraded"
        
        # Check RAG service
        if rag_service and hasattr(rag_service, 'knowledge_documents'):
            health_status["services"]["rag_service"] = "healthy"
            health_status["knowledge_documents"] = len(rag_service.knowledge_documents)
        else:
            health_status["services"]["rag_service"] = "unhealthy"
            health_status["status"] = "degraded"
        
        return health_status
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/stats")
async def get_statistics():
    """Get AZOM pipeline statistics."""
    
    try:
        stats = {
            "server": {
                "name": settings.APP_NAME,
                "version": settings.APP_VERSION,
                "uptime": "calculated_at_runtime",  # Would calculate actual uptime
                "model_target": settings.TARGET_MODEL
            },
            "knowledge_base": {},
            "performance": {
                "max_concurrent": settings.MAX_CONCURRENT_REQUESTS,
                "timeout": settings.REQUEST_TIMEOUT,
                "cache_ttl": settings.CACHE_TTL
            }
        }
        
        # Get RAG statistics
        if rag_service:
            stats["knowledge_base"] = rag_service.get_knowledge_statistics()
        
        return stats
        
    except Exception as e:
        logger.error(f"Stats error: {e}")
        raise HTTPException(status_code=500, detail="Failed to get statistics")

# OpenAI Models endpoint (for compatibility)
@app.get("/v1/models")
async def list_models():
    """List available models (OpenAI API compatibility)."""
    return {
        "object": "list",
        "data": [
            {
                "id": "azom-se-general",
                "object": "model",
                "created": int(datetime.now().timestamp()),
                "owned_by": "azom-pipeline",
                "root": "azom-se-general",
                "parent": None,
                "permission": []
            }
        ]
    }

# Error handlers
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global exception on {request.url.path}: {exc}", exc_info=True)
    
    return JSONResponse(
        status_code=500,
        content={
            "error": "AZOM_PIPELINE_ERROR",
            "message": "Ett ov√§ntat fel uppstod i AZOM pipeline-servern. Support kontaktad.",
            "support": "support@azom.se",
            "timestamp": datetime.now().isoformat()
        }
    )

if __name__ == "__main__":
    uvicorn.run(
        "app.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level=settings.LOG_LEVEL.lower()
    )
```


## **requirements.txt**

```
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
httpx==0.25.2
langchain==0.1.0
langchain-community==0.0.10
sentence-transformers==2.2.2
scikit-learn==1.3.2
pandas==2.1.4
numpy==1.24.3
aiofiles==23.2.1
python-multipart==0.0.6
python-dotenv==1.0.0
```


## **docker/Dockerfile**

```dockerfile
FROM python:3.11-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV DEBIAN_FRONTEND=noninteractive

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Download Swedish language model
RUN python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('KBLab/sentence-bert-swedish-cased')"

# Copy application code
COPY . .

# Create directories
RUN mkdir -p data/knowledge_base data/embeddings data/cache logs

# Create non-root user
RUN useradd -m -u 1000 azom && \
    chown -R azom:azom /app
USER azom

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8001/health || exit 1

# Expose port
EXPOSE 8001

# Start command
CMD ["python", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8001"]
```


## **docker/docker-compose.yml**

```yaml
version: '3.8'

services:
  azom-pipeline-server:
    build: 
      context: ..
      dockerfile: docker/Dockerfile
    container_name: azom-pipeline-server
    ports:
      - "8001:8001"
    environment:
      - OPENAI_API_BASE=http://titanic.urem.org:3000
      - OPENAI_API_KEY=${OPENWEBUI_API_TOKEN}
      - TARGET_MODEL=azom-se-general
      - DEBUG=false
      - LOG_LEVEL=INFO
    volumes:
      - ../data:/app/data
      - ../logs:/app/logs
      - pipeline-cache:/app/data/cache
    restart: unless-stopped
    networks:
      - azom-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  pipeline-cache:

networks:
  azom-network:
    driver: bridge
```


## **Deployment och Test**

### **Starta Pipeline Server**

```bash
# S√§tt upp environment
echo "OPENWEBUI_API_TOKEN=your_token_here" > .env

# Starta med Docker
docker-compose up -d

# Eller lokalt f√∂r utveckling
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
```


### **Registrera i OpenWebUI**

```bash
# 1. G√• till OpenWebUI: http://titanic.urem.org:3000
# 2. Settings > Models > Add Model
# 3. Model Name: azom-pipeline
# 4. Base URL: http://localhost:8001/v1
# 5. API Key: (samma som OpenWebUI om auth kr√§vs)
```


### **Test Pipeline**

```bash
# Testa direkt
curl -X POST "http://localhost:8001/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "azom-se-general",
    "messages": [
      {"role": "user", "content": "Jag har en Golf V fr√•n 2008 och vill installera AZOM. Jag √§r nyb√∂rjare."}
    ],
    "temperature": 0.3
  }'

# Health check
curl http://localhost:8001/health

# Statistics
curl http://localhost:8001/stats
```


## **Sammanfattning**

Den kompletta AZOM Pipeline Server √§r nu redo! Den inkluderar:

‚úÖ **OpenAI API-kompatibilitet** f√∂r seamless OpenWebUI-integration
‚úÖ **Avancerad orchestration** med LangChain f√∂r komplexa arbetsfl√∂den
‚úÖ **RAG-implementation** med svensk spr√•kst√∂d och embeddings
‚úÖ **Memory-hantering** f√∂r anv√§ndarhistorik och personalisering
‚úÖ **S√§kerhetsvalidering** med proaktiva varningar
‚úÖ **Performance-optimering** med caching och concurrent processing
‚úÖ **Docker-deployment** f√∂r enkel produktionss√§ttning
‚úÖ **Komplett logging och monitoring**

**Pipeline-servern √§r nu redo f√∂r produktionsanv√§ndning och ger AZOM:s AI-agent avancerade m√∂jligheter utan att begr√§nsa OpenWebUI:s funktionalitet!** üöÄ

